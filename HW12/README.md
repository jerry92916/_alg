首先將隨機生成的Logits透過Softmax函數轉換為符合機率定義的預測分佈 $q$，接著利用Cross Entropy衡量 $q$ 與目標分佈 $p$ 之間的差距。  
在訓練循環中，計算出損失函數對Logits的梯度（即誤差 $q - p$），並以此方向利用梯度下降法微調Logits。  
經過多次迭代修正，預測分佈 $q$ 會不斷逼近真實分佈 $p$，最終使損失函數達到極小值。  
[程式碼](https://github.com/jerry92916/_alg/blob/main/HW12/HW12.py)

---
執行結果
```
PS C:\Users\宏傑\OneDrive\桌面\alg>  c:; cd 'c:\Users\宏傑\OneDrive\桌面\alg'; & 'c:\Users\宏傑\AppData\Local\Programs\Python\Python310\python.exe' 'c:\Users\宏傑\.vscode\extensions\ms-python.debugpy-2025.18.0-win32-x64\bundled\libs\debugpy\launcher' '54712' '--' 'C:\Users\宏傑\OneDrive\桌面\alg\HW12.py'
Step 0
q = [0.1539486  0.23904831 0.60700309]
Cross Entropy = 1.2395385302472908
----------------------------------------
Step 100
q = [0.19533309 0.4968978  0.30776911]
Cross Entropy = 1.0298168848123204
----------------------------------------
Step 200
q = [0.19946196 0.500078   0.30046004]
Cross Entropy = 1.0296540975383242
----------------------------------------
Step 300
q = [0.19994681 0.50001428 0.30003891]
Cross Entropy = 1.029653023864049
----------------------------------------
Step 400
q = [0.19999488 0.50000149 0.30000363]
Cross Entropy = 1.0296530141513613
----------------------------------------
Step 500
q = [0.19999951 0.50000015 0.30000035]
Cross Entropy = 1.0296530140623963
----------------------------------------
Step 600
q = [0.19999995 0.50000001 0.30000003]
Cross Entropy = 1.0296530140615812
----------------------------------------
Step 700
q = [0.2 0.5 0.3]
Cross Entropy = 1.0296530140615738
----------------------------------------
Step 800
q = [0.2 0.5 0.3]
Cross Entropy = 1.0296530140615736
----------------------------------------
Step 900
q = [0.2 0.5 0.3]
Cross Entropy = 1.0296530140615736
----------------------------------------
Final q: [0.2 0.5 0.3]
True p : [0.2 0.5 0.3]
```
